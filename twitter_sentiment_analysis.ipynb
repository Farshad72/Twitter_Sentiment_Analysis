{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farshad72/Twitter_Sentiment_Analysis/blob/main/twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine Tuning Bert-base-uncased for sentiment analysis**\n",
        "This is example of finetuning bert-base-uncased for sentiment classification using twitter data. The first step is to import the required packages. Keep in mind that you have to install them first."
      ],
      "metadata": {
        "id": "aDeqf45UYQj6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrsmDraoqP7u"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing the Dataset**\n",
        "We use load_dataset function to import data from the huggingface. Since the origina \"tweet_eval\" dataset is large (45k rows), we only download 20% of the dataset."
      ],
      "metadata": {
        "id": "QUsG6eEwbL0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gHxYHIOqpWa"
      },
      "outputs": [],
      "source": [
        "sentiment_dataset = load_dataset(\"tweet_eval\", \"sentiment\", split=\"train[:20%]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Splitting the Data**\n",
        "The process of splitting the data into train, test and validation set is as follows. First, we devide the hole data set into 60% of training set and 40% of primary test set. Then, the split the primary test set into 50% of test set and 50% validation set. Hence, the outcome is 60% of training set and 20% of test and validation sets respectively."
      ],
      "metadata": {
        "id": "J3YysFHLcXJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSJALtT9Odj4"
      },
      "outputs": [],
      "source": [
        "new_dataset= sentiment_dataset.train_test_split(test_size=0.4)\n",
        "new_dataset_2 = new_dataset['test'].train_test_split(test_size=0.5)\n",
        "train_set = new_dataset['train']\n",
        "test_set = new_dataset_2['test']\n",
        "val_set = new_dataset_2['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TextClassificationDataset Class**\n",
        "This class inherit from pytorch Dataset class [details](https://pytorch.org/docs/stable/data.html) and it is suitable for text classification task. Overall, the TextClassificationDataset class encode the input data with its label and pass it for training with its attention mask. We set the output tensor in pytorch format bacause we will use pytorch for training. We set the paddind and truncation true to prevent introducing erorr to training due to long or short texts."
      ],
      "metadata": {
        "id": "Si19ypoodIe5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfwQsEHPj--T"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BertClassification Class**\n",
        "This class inherit form the torch.Module class [(details)](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and specify the neural network model in pytorch environment. This class add a linear clasification layer to the bert model. We set the dropout probability to 0.2, meaning the neurons are ignored during training independently with probaility of 20%. The self.fc function Initializes a fully connected layer responsible for mapping the BERT model's output to the desired number of classes. The class output is the logits of each class, which is unnormalized propabbility and need to be transformed later to reflect the probability of each class for any input data."
      ],
      "metadata": {
        "id": "K4bgzSH13nFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOEc_ka_kLCD"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = self.fc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Phase**\n",
        "\n",
        "In this porject, we define a function for model traiing. Alternatively, you can use huggignface trainer function [(details)](https://huggingface.co/docs/transformers/en/main_classes/trainer). The model.train() argument set the model in the training mode. The optimizer.zero_grad clear gradients from previous iterations to avoid accumulating gradients through multiple backpropagation steps. The reaming  steps calcualte the loss and pass it to the training phase and update the model's parameters based on the calculated gradients, adjusting weights to reduce loss in subsequent iterations."
      ],
      "metadata": {
        "id": "TWjfjXdk911R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO4sYaNwkb2o"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Evaluation Function**\n",
        "This function is designed to evaluate the performance of a text classification model on a given dataset. We will use this function both in the training part and for predicting the test set sentiment.\n"
      ],
      "metadata": {
        "id": "doYOqDgf_qjy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBWuq1f0nYih"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.to(device).tolist())\n",
        "            actual_labels.extend(labels.to(device).tolist())\n",
        "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Prediction Function**\n",
        "\n",
        "This function is designed to predict the sentiment of any text. The difference between this function and the evaluate function lies in their input and output.\n",
        "\n",
        "|function|input|output|\n",
        "|:---|:---|:---|\n",
        "| evaluate | Pytorch DataLoader class | accuracy_score and classification_report |\n",
        "| predict_sentiment | text (string) | input sentiment|"
      ],
      "metadata": {
        "id": "5fmGpsxsAS-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNQ7qoSunfoQ"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "    return preds.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model and Hyperparameters**\n",
        "we set the model as bert-base-uncased and define the hyperparameter. These parameters should be iterated to find the optimum outcome. We will run the model with different sets of parameters in the finetuning step."
      ],
      "metadata": {
        "id": "UVcz2p4MCEy7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtgzgeR_n7rK"
      },
      "outputs": [],
      "source": [
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = 3\n",
        "max_length = 128\n",
        "batch_size = 32\n",
        "num_epochs = 4\n",
        "learning_rate = 3e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setting up the Tokenizer and Preparing the Data**\n",
        "\n",
        "We set the tokenizer as BertTokenier, suitbale for tokenizing text for bert models. The training and validation data is encoded with the TextClassification class which defined earlier. Finally, the data should be converted for batch processing using DataLoader function."
      ],
      "metadata": {
        "id": "eU8kyz_zF5bu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-KFoPFvoHD6"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "train_dataset = TextClassificationDataset(train_set['text'], train_set['label'], tokenizer, max_length)\n",
        "val_dataset = TextClassificationDataset(val_set['text'], val_set['label'], tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setting the Device**\n",
        "Finetuning the LLMS require heavy computation. Hence, the priority is to use GPU or TPU."
      ],
      "metadata": {
        "id": "roeEYg3dGkOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gv_2fkjso17o"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optimizer, Steps and Scheduler**\n",
        "\n",
        "- creates an optimizer object using the AdamW algorithm (```torch.optim.AdamW```). AdamW is a popular optimizer choice in deep learning, known for its efficiency and effectiveness.\n",
        "\n",
        "- This line calculates the total number of training steps the model will go through.\n",
        "len(train_dataloader): This provides the number of batches in your training data loader.\n",
        "num_epochs: This represents the number of times you plan to iterate through the entire training dataset.\n",
        "Multiplying these two values gives you the total number of steps (iterations) your training loop will execute. This information is crucial for the learning rate scheduler we'll create next.\n",
        "\n",
        "- This line creates a learning rate scheduler object using the get_linear_schedule_with_warmup function (likely from a library like transformers from\n",
        "```Huggingface```\n",
        ").\n",
        "A learning rate scheduler is a technique for adjusting the learning rate during training. Here, we're using a linear scheduler with warmup.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3L8x9j9WG3br"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E34HZwMVo6vO"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Early stopping**\n"
      ],
      "metadata": {
        "id": "1es2e_XUmJpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class early_stopping:\n",
        "  def __init__(self, patience=3, min_delta=0.01):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "  def __call__(self, val_loss):\n",
        "    if self.counter >= self.patience:\n",
        "      return True\n",
        "    elif val_loss < self.best_val_loss - self.min_delta:\n",
        "      self.best_val_loss = val_loss\n",
        "      self.counter = 0\n",
        "    else:\n",
        "      self.counter += 1\n",
        "    return False\n",
        "\n"
      ],
      "metadata": {
        "id": "V7pbJ4Z3mO4Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterates through Training Epoch \"\"\n",
        "This code iterates through training epochs, performs training and evaluation within each epoch, and provides informative feedback on training progress through loss values and detailed evaluation metrics on the validation data."
      ],
      "metadata": {
        "id": "oTMq1TkzJtla"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V533m_iSpFkN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "35c759c9-63c3-4fdf-ad21-d218b5d894d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-98ac1a698720>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
          ]
        }
      ],
      "source": [
        "early_stopping_callback = early_stopping()\n",
        "for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        train(model, train_dataloader, optimizer, scheduler, device)\n",
        "        val_loss = 0.0\n",
        "        train_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "              input_ids = batch['input_ids'].to(device)\n",
        "              attention_mask = batch['attention_mask'].to(device)\n",
        "              labels = batch['label'].to(device)\n",
        "              outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "              loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "              val_loss += loss.item()\n",
        "            for batch in train_dataloader:\n",
        "              input_ids = batch['input_ids'].to(device)\n",
        "              attention_mask = batch['attention_mask'].to(device)\n",
        "              labels = batch['label'].to(device)\n",
        "              outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "              loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "              train_loss += loss.item()\n",
        "        train_loss /= len(train_dataloader)\n",
        "        val_loss /= len(val_dataloader)\n",
        "        if early_stopping_callback(val_loss):\n",
        "          print(f\"Early stopping at epoch:{epoch+1}\")\n",
        "          break\n",
        "        print(f\"Training Loss: {train_loss:.4f}\")  # Print validation loss\n",
        "        print(f\"Validation Loss: {val_loss:.4f}\")  # Print validation loss\n",
        "        accuracy, report = evaluate(model, val_dataloader, device)\n",
        "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "        print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance on Test set\n",
        "\n",
        "Finally, we check the model performance on the unseen test set. The test set should first preprocessed (encode, batching, truncation, etc.) using DataLoader library. The evaluation function provide the detaield performacne metrics for the entire test set."
      ],
      "metadata": {
        "id": "6B3EGDDPKH2C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j1UkiqFt4dn"
      },
      "outputs": [],
      "source": [
        "test_dataset = TextClassificationDataset(test_set['text'], test_set['label'], tokenizer, max_length)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "evaluate(model, test_dataloader, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameter Finetuning**\n",
        "\n",
        "variables:\n",
        "\n",
        "|parameter|description|\n",
        "|:---|:---|\n",
        "| learning rate | Adjusts step size in gradient descent optimization algorithm |\n",
        "| batch size |Number of samples processed before updating model parameters |\n",
        "| dropout | Randomly deactivates neurons during training to prevent overfitting |\n",
        "| weight decay |  Regularization technique to penalize large weight values |\n",
        "| warm up steaps ratio | Ratio of total stapes that learning rate gradually increases initially to reach its value |\n",
        "| max length | Maximum length of input sequence for model processing |\n",
        "\n",
        "Hyperparameter finetuning parameters:\n",
        "\n",
        "|Model number|learning rate|batch size|dropout|weight decay|warm up steps|max length|\n",
        "|:---|:---|:---|:---|:---|:---|:---|\n",
        "|1|5e-5|32|0.5|0|0|128|\n",
        "|2|1e-5|32|0.5|0|0|128|\n",
        "|3|5e-6|32|0.5|0|0|128|\n",
        "|4|5e-6|32|0.5|0|0.1|128|\n",
        "|5|5e-6|64|0.2|0|0|128|\n",
        "|6|5e-6|32|0.4|0|0|128|\n",
        "|7|5e-6|32|0.4|0.1|0|128|\n",
        "|8|5e-6|32|0.2|0.2|0|128|\n",
        "|9|5e-6|32|0.4|0.3|0|128|\n",
        "|10|5e-6|64|0.4|0.2|0|128|\n",
        "|11|5e-6|64|0.4|0.2|0.1|128|\n",
        "|12|5e-6|32|0.4|0.2|0|256|\n",
        "   \n",
        "  "
      ],
      "metadata": {
        "id": "731Z-e7SD6E0"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "https://github.com/Farshad72/Twitter_Sentiment_Analysis/blob/main/twitter_sentiment_analysis.ipynb",
      "authorship_tag": "ABX9TyM7sncjA20W/Usx0sXW+SCZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}